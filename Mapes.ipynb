{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8ZNNXRwxJ9L"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kg--Mw_xeuKa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from itertools import combinations\n",
    "import sys\n",
    "\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union, nearest_points\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import folium\n",
    "from folium import GeoJson, LayerControl\n",
    "from branca.colormap import linear\n",
    "import branca.colormap as cm\n",
    "\n",
    "import igraph as ig\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_flooded_roads(edges, nodes, flood_zones, name):\n",
    "    output_path = cut_roads_files[name]\n",
    "    graphml_path = safe_roads_files[name]\n",
    "\n",
    "    #if os.path.exists(output_path) and layer in fiona.listlayers(output_path):\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Loading {name} from {output_path}\")\n",
    "        G_safe = ox.load_graphml(graphml_path)\n",
    "        edges = gpd.read_file(output_path, layer=name)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Tagging and saving {name} to {output_path}\")\n",
    "\n",
    "        bounds = edges.total_bounds\n",
    "        flood_subset = flood_zones.cx[bounds[0]:bounds[2], bounds[1]:bounds[3]]\n",
    "        flood_geoms = flood_subset.geometry\n",
    "\n",
    "        edges = edges.copy()\n",
    "        edges[\"in_flood_zone\"] = edges.geometry.apply(lambda geom: flood_geoms.intersects(geom).any())\n",
    "\n",
    "        edges.to_file(output_path, layer=name, driver=\"GPKG\")\n",
    "\n",
    "    if os.path.exists(graphml_path):\n",
    "        print(\"Pruned graph already exists\")\n",
    "    else:    \n",
    "        safe_edges = edges[~edges[\"in_flood_zone\"]].copy()\n",
    "        print(\"Rebuilding pruned graph...\")\n",
    "        G_safe = ox.graph_from_gdfs(nodes, safe_edges)\n",
    "        ox.save_graphml(G_safe, filepath=graphml_path)\n",
    "        print(f\"Saved pruned graph to {graphml_path}\")\n",
    "\n",
    "    return edges, G_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_flood_zone(return_crs, name, clip_geom):\n",
    "    output_path = zone_output_files[name]\n",
    "    input_path = zone_input_files[name]\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Loading {name} from {output_path}\")\n",
    "        clipped = gpd.read_file(output_path, layer=name).to_crs(return_crs)\n",
    "    else:\n",
    "        print(f\"Clipping and saving {name} from {output_path}\" )\n",
    "        flood = gpd.read_file(input_path).to_crs(return_crs)\n",
    "        clipped = gpd.clip(flood, clip_geom)\n",
    "        clipped.to_file(output_path, layer=name, driver=\"GPKG\")\n",
    "\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_geojson_safe(gdf):\n",
    "    gdf = gdf.copy()\n",
    "    dt_cols = gdf.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns\n",
    "    gdf[dt_cols] = gdf[dt_cols].astype(str)\n",
    "    for col in gdf.columns:\n",
    "        if col != \"geometry\" and not pd.api.types.is_scalar(gdf[col].iloc[0]):\n",
    "            gdf.drop(columns=[col], inplace=True)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_flood_zone_layer(name, m):\n",
    "    gdf=flood_zones_var[name]\n",
    "    color=color_palette[name]\n",
    "    if gdf.crs.to_epsg() != 4326:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    gdf_serializable = make_geojson_safe(gdf)\n",
    "\n",
    "    style_function = lambda x: {\n",
    "        'fillColor': color,\n",
    "        'color': color,\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    "\n",
    "    geojson = folium.GeoJson(\n",
    "        data=gdf_serializable,\n",
    "        name=f\"Flood {name}\",\n",
    "        style_function=style_function,\n",
    "        show=False\n",
    "    )\n",
    "    geojson.add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_roads_layer(name, m, flood):\n",
    "    if flood:\n",
    "        roads = flood_edges_var[name].copy()\n",
    "        roads = roads[roads[\"in_flood_zone\"] == flood]\n",
    "    else:\n",
    "        roads = edges.copy()\n",
    "\n",
    "    roads = roads.to_crs(epsg=4326)\n",
    "    roads = make_geojson_safe(roads)\n",
    "\n",
    "    style_function = lambda x: {\n",
    "        'color': color_palette[name],\n",
    "        'weight': 2,\n",
    "        'opacity': 0.6\n",
    "    }\n",
    "\n",
    "    if flood == True:\n",
    "        geojson = folium.GeoJson(\n",
    "            roads,\n",
    "            name=f\"Flooded Roads {name}\",\n",
    "            style_function=style_function,\n",
    "            show=False\n",
    "        )\n",
    "    else:\n",
    "            geojson = folium.GeoJson(\n",
    "            roads,\n",
    "            name=f\"All roads\",\n",
    "            style_function=style_function,\n",
    "            show=False\n",
    "        )\n",
    "    \n",
    "    geojson.add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_roads_layer_risk(name, m, flood):\n",
    "    if flood:\n",
    "        roads = flood_edges_var[name].copy()\n",
    "        roads = roads[roads[\"in_flood_zone\"] == flood]\n",
    "    else:\n",
    "        roads = edges.copy()\n",
    "\n",
    "    roads = roads.to_crs(epsg=4326)\n",
    "    roads = make_geojson_safe(roads)\n",
    "\n",
    "    style_function = lambda x: {\n",
    "        'color': color_palette[name],\n",
    "        'weight': 2,\n",
    "        'opacity': 0.6\n",
    "    }\n",
    "\n",
    "    if flood == True:\n",
    "        geojson = folium.GeoJson(\n",
    "            roads,\n",
    "            name=f\"Flooded Roads {name}\",\n",
    "            style_function=style_function,\n",
    "            show=False\n",
    "        )\n",
    "    else:\n",
    "            geojson = folium.GeoJson(\n",
    "            roads,\n",
    "            name=f\"All roads\",\n",
    "            style_function=style_function,\n",
    "            show=False\n",
    "        )\n",
    "    \n",
    "    geojson.add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depth_range(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "\n",
    "    val = val.strip()\n",
    "\n",
    "    if val.startswith('Below'):\n",
    "        return float(val[5:].strip()) / 2\n",
    "\n",
    "    if val.startswith('>'):\n",
    "        return float(val[1:].strip())  # You may want to cap it\n",
    "\n",
    "    if '-' in val:\n",
    "        parts = val.split('-')\n",
    "        try:\n",
    "            low = float(parts[0].strip())\n",
    "            high = float(parts[1].strip())\n",
    "            return (low + high) / 2\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_individual_risk_factor(T_P, T_NP):\n",
    "    if T_P == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 - (T_NP / T_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_factor(T_P_dict, T_NP_dict):\n",
    "    keys = set(T_P_dict.keys()) & set(T_NP_dict.keys())\n",
    "    R = 0\n",
    "    for k in keys:\n",
    "        T_P_time = T_P_dict[k][1]\n",
    "        T_NP_time = T_NP_dict[k][1]\n",
    "        R += compute_individual_risk_factor(T_P_time, T_NP_time)\n",
    "    R /= len(keys) if keys else 1\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_factor_2(T_P_dict, T_NP_dict): #when not computing paths\n",
    "    keys = set(T_P_dict.keys()) & set(T_NP_dict.keys())\n",
    "    R = 0\n",
    "    for k in keys:\n",
    "        T_P_time = T_P_dict[k]\n",
    "        T_NP_time = T_NP_dict[k][1]\n",
    "        R += compute_individual_risk_factor(T_P_time, T_NP_time)\n",
    "    R /= len(keys) if keys else 1\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flood_depth_zones(name):\n",
    "    layer=\"depth_val\"\n",
    "    input_path = depth_input_files[name]\n",
    "    output_path=depth_output_files[name]\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Loading {layer} from {output_path}\")\n",
    "        depth=gpd.read_file(output_path, layer=layer)\n",
    "    else:\n",
    "        print(f\"Saving {layer} to {output_path}\")\n",
    "        depth = gpd.read_file(input_path)\n",
    "        depth[\"depth_val\"] = depth[\"value\"].apply(parse_depth_range)\n",
    "        depth.to_file(output_path, layer=layer, driver=\"GPKG\")\n",
    "        print(f\"Saved processed {layer} in {output_path}\")\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nx_to_igraph(G_nx, weight_attr='travel_time'):\n",
    "    node_list = list(G_nx.nodes())\n",
    "    node_to_index = {node: i for i, node in enumerate(node_list)}\n",
    "    index_to_node = {i: node for node, i in node_to_index.items()}\n",
    "\n",
    "    edge_weights = {}\n",
    "    for u, v, attr in G_nx.edges(data=True):\n",
    "        wt = attr.get(weight_attr)\n",
    "        if wt is None:\n",
    "            continue\n",
    "        u_idx, v_idx = node_to_index[u], node_to_index[v]\n",
    "        if (u_idx, v_idx) not in edge_weights or wt < edge_weights[(u_idx, v_idx)]:\n",
    "            edge_weights[(u_idx, v_idx)] = wt\n",
    "\n",
    "    is_directed = G_nx.is_directed()\n",
    "    G_ig = ig.Graph(directed=is_directed)\n",
    "    G_ig.add_vertices(len(node_list))\n",
    "    G_ig.add_edges(edge_weights.keys())\n",
    "    G_ig.es['weight'] = list(edge_weights.values())\n",
    "\n",
    "    return G_ig, node_to_index, index_to_node, edge_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_shortest_paths(G_ig, special_nodes, index_to_node, node_to_index, node_to_muni, compute_path, weight_attr='weight'):\n",
    "    result = {}\n",
    "    special_indices = [node_to_index[n] for n in special_nodes]\n",
    "    index_to_muni = {idx: node_to_muni[index_to_node[idx]] for idx in special_indices}\n",
    "\n",
    "    for src_idx in special_indices:\n",
    "        distances = G_ig.distances(source=src_idx, target=special_indices, weights=weight_attr)[0]\n",
    "        paths = None\n",
    "        if compute_path:\n",
    "            paths = G_ig.get_shortest_paths(src_idx, to=special_indices, weights=weight_attr, output='vpath')\n",
    "\n",
    "        for tgt_pos, tgt_idx in enumerate(special_indices):\n",
    "            if src_idx == tgt_idx:\n",
    "                continue\n",
    "\n",
    "            dist = distances[tgt_pos]\n",
    "            if math.isinf(dist):\n",
    "                result[f\"{index_to_muni[src_idx]}__{index_to_muni[tgt_idx]}\"] = ([], 0) if compute_path else 0\n",
    "                continue\n",
    "\n",
    "            if compute_path:\n",
    "                node_path = paths[tgt_pos]\n",
    "                path = [index_to_node[n] for n in node_path] if node_path else []\n",
    "                result[f\"{index_to_muni[src_idx]}__{index_to_muni[tgt_idx]}\"] = (path, dist)\n",
    "            else:\n",
    "                result[f\"{index_to_muni[src_idx]}__{index_to_muni[tgt_idx]}\"] = dist\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_shortest_paths_no_path(G_ig, special_nodes, index_to_node, node_to_index, node_to_muni, weight_attr='weight'):\n",
    "    result = {}\n",
    "\n",
    "    special_indices = [node_to_index[n] for n in special_nodes]\n",
    "    index_to_muni = {idx: node_to_muni[index_to_node[idx]] for idx in special_indices}\n",
    "\n",
    "    dist_matrix = G_ig.shortest_paths_dijkstra(\n",
    "        source=special_indices,\n",
    "        target=special_indices,\n",
    "        weights=weight_attr\n",
    "    )\n",
    "\n",
    "    for i, src_idx in enumerate(special_indices):\n",
    "        for j, tgt_idx in enumerate(special_indices):\n",
    "            if src_idx == tgt_idx:\n",
    "                continue\n",
    "\n",
    "            dist = dist_matrix[i][j]\n",
    "            key = f\"{index_to_muni[src_idx]}__{index_to_muni[tgt_idx]}\"\n",
    "            result[key] = 0 if math.isinf(dist) else dist\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_compute_shortest_paths(filename, G_nx, special_nodes, node_to_muni, save, compute_path):\n",
    "    G_ig, node_to_index, index_to_node, edge_weights = convert_nx_to_igraph(G_nx, weight_attr='travel_time')\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Deserialize: convert lists to tuples if compute_path is True\n",
    "        if compute_path:\n",
    "            result = {k: (v[0], v[1]) for k, v in data.items()}\n",
    "        else:\n",
    "            result = data  # just distances\n",
    "    else:\n",
    "        print(\"Computing shortest paths...\")\n",
    "        result = batch_shortest_paths(\n",
    "            G_ig,\n",
    "            special_nodes,\n",
    "            index_to_node,\n",
    "            node_to_index,\n",
    "            node_to_muni,\n",
    "            compute_path=compute_path\n",
    "        )\n",
    "\n",
    "        if save:\n",
    "            if compute_path:\n",
    "                # Convert tuples to lists for JSON compatibility\n",
    "                serializable_result = {k: [v[0], v[1]] for k, v in result.items()}\n",
    "            else:\n",
    "                # Values are just floats\n",
    "                serializable_result = result\n",
    "\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(serializable_result, f)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae_V4hjCAsN8"
   },
   "source": [
    "# Street data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"processed_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CPYUcbX01Hvy"
   },
   "outputs": [],
   "source": [
    "ox.settings.use_cache = True\n",
    "ox.settings.log_console = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "polygon_path = os.path.join(output_dir, \"study_area.geojson\")\n",
    "graph_path = os.path.join(output_dir, \"road_graph.graphml\")\n",
    "\n",
    "valencia_municipalities = [\n",
    "    \"Alaquàs\", \"Albal\", \"Albalat de la Ribera\", \"Alberic\", \"Alborache\", \"Alcàsser\", \"l'Alcúdia\",\n",
    "    \"Aldaia\", \"Alfafar\", \"Alfarb\", \"Algemesí\", \"Alginet\", \"Almussafes\", \"Alzira\",\n",
    "    \"Benetússer\", \"Benicull de Xúquer\", \"Benifaió\", \"Beniparrell\", \"Benimodo\", \"Bétera\",\n",
    "    \"Bugarra\", \"Buñol\", \"Calles\", \"Camporrobles\", \"Carcaixent\", \"Carlet\", \"Catadau\", \"Catarroja\",\n",
    "    \"Caudete de las Fuentes\", \"Chera\", \"Cheste\", \"Chiva\", \"Chulilla\", \"Corbera\", \"Cullera\",\n",
    "    \"Dos Aguas\", \"Favara\", \"Fortaleny\", \"Fuenterrobles\", \"Gestalgar\", \"Godelleta\", \"Guadassuar\",\n",
    "    \"l'Ènova\", \"Llaurí\", \"Llombai\", \"Llíria\", \"Llocnou de la Corona\", \"Loriguilla\", \"Macastre\",\n",
    "    \"Manises\", \"Manuel\", \"Massanassa\", \"Millares\", \"Mislata\", \"Montroi\", \"Montserrat\", \"Paiporta\",\n",
    "    \"Paterna\", \"Pedralba\", \"Picanya\", \"Picassent\", \"Polinyà de Xúquer\", \"La Pobla Llarga\",\n",
    "    \"Quart de Poblet\", \"Rafelguaraf\", \"Real\", \"Requena\", \"Riba-roja de Túria\", \"Riola\", \"Sedaví\",\n",
    "    \"Senyera\", \"Siete Aguas\", \"Silla\", \"Sinarcas\", \"Sollana\", \"Sot de Chera\", \"Sueca\",\n",
    "    \"Tavernes de la Valldigna\", \"Torrent\", \"Tous\", \"Turís\", \"Utiel\", \"València\", \"Vilamarxant\", \"Xirivella\",\n",
    "    \"Yátova\"\n",
    "]\n",
    "\n",
    "urban_center={\n",
    "    \"Alaquàs\": (39.457119, -0.460822), \n",
    "    \"Albal\": (39.397347, -0.413243), \n",
    "    \"Albalat de la Ribera\": (39.201133, -0.386659),\n",
    "    \"Alberic\": (39.116783, -0.517571),\n",
    "    \"Alborache\": (39.391716, -0.771191),\n",
    "    \"Alcàsser\": (39.369810, -0.445176),\n",
    "    \"l'Alcúdia\": (39.853110, 3.121137),\n",
    "    \"Aldaia\": (39.464957, -0.460764),\n",
    "    \"Alfafar\": (39.422773, -0.390834), \n",
    "    \"Alfarb\": (39.276849, -0.560529),\n",
    "    \"Algemesí\": (39.188743, -0.436927),\n",
    "    \"Alginet\": (39.261819, -0.469791),\n",
    "    \"Almussafes\": (39.292656, -0.413260),\n",
    "    \"Alzira\": (39.152038, -0.441074),\n",
    "    \"Benetússer\": (39.423801, -0.397785),\n",
    "    \"Benicull de Xúquer\": (39.185063, -0.382524), \n",
    "    \"Benifaió\": (39.285003, -0.426889), \n",
    "    \"Beniparrell\": (39.382235, -0.412277), \n",
    "    \"Benimodo\": (39.213015, -0.528357), \n",
    "    \"Bétera\": (39.591643, -0.462355),\n",
    "    \"Bugarra\": (39.608415, -0.775958), \n",
    "    \"Buñol\": (39.418217, -0.790672), \n",
    "    \"Calles\": (39.725371, -0.973952), \n",
    "    \"Camporrobles\": (39.647167, -1.399476), \n",
    "    \"Carcaixent\": (39.122600, -0.450718), \n",
    "    \"Carlet\": (39.225881, -0.519809), \n",
    "    \"Catadau\": (39.275363, -0.569740), \n",
    "    \"Catarroja\": (39.403048, -0.404021),\n",
    "    \"Caudete de las Fuentes\": (39.558958, -1.278492), \n",
    "    \"Chera\": (39.593442, -0.973241), \n",
    "    \"Cheste\": (39.494909, -0.684261), \n",
    "    \"Chiva\": (39.471469, -0.717100), \n",
    "    \"Chulilla\": (39.656217, -0.891905), \n",
    "    \"Corbera\": (39.158264, -0.355378), \n",
    "    \"Cullera\": (39.165250, -0.253612),\n",
    "    \"Dos Aguas\": (39.288825, -0.800238), \n",
    "    \"Favara\": (39.127275, -0.291808), \n",
    "    \"Fortaleny\": (39.183833, -0.313715), \n",
    "    \"Fuenterrobles\": (39.584682, -1.364031), \n",
    "    \"Gestalgar\": (39.604207, -0.834346), \n",
    "    \"Godelleta\": (39.422040, -0.686434), \n",
    "    \"Guadassuar\": (39.185852, -0.478023),\n",
    "    \"l'Ènova\": (39.045134, -0.480790), \n",
    "    \"Llaurí\": (39.147102, -0.330235), \n",
    "    \"Llombai\": (39.282414, -0.572366), \n",
    "    \"Llíria\": (39.624719, -0.595006), \n",
    "    \"Llocnou de la Corona\": (39.420446, -0.382216), \n",
    "    \"Loriguilla\": (39.489698, -0.571964), \n",
    "    \"Macastre\": (39.381938, -0.785287),\n",
    "    \"Manises\": (39.493345, -0.457466), \n",
    "    \"Manuel\": (39.052760, -0.494055), \n",
    "    \"Massanassa\": (39.411460, -0.397986), \n",
    "    \"Millares\": (39.238985, -0.773203), \n",
    "    \"Mislata\": (39.475218, -0.417833), \n",
    "    \"Montroi\": (39.341742, -0.613689), \n",
    "    \"Montserrat\": (39.358012, -0.603212), \n",
    "    \"Paiporta\": (39.429588, -0.417467),\n",
    "    \"Paterna\": (39.500663, -0.439682), \n",
    "    \"Pedralba\": (39.604864, -0.726535), \n",
    "    \"Picanya\": (39.435146, -0.433490), \n",
    "    \"Picassent\": (39.362746, -0.458078), \n",
    "    \"Polinyà de Xúquer\": (39.196150, -0.369461), \n",
    "    \"La Pobla Llarga\": (39.085916, -0.475557),\n",
    "    \"Quart de Poblet\": (39.483003, -0.442288), \n",
    "    \"Rafelguaraf\": (39.050797, -0.455126), \n",
    "    \"Real\": (39.335088, -0.609125), \n",
    "    \"Requena\": (39.487037, -1.098087), \n",
    "    \"Riba-roja de Túria\": (39.546926, -0.566891), \n",
    "    \"Riola\": (39.195261, -0.334682), \n",
    "    \"Sedaví\": (39.425005, -0.385783),\n",
    "    \"Senyera\": (39.063613, -0.510467), \n",
    "    \"Siete Aguas\": (39.471633, -0.915872), \n",
    "    \"Silla\": (39.362679, -0.412147), \n",
    "    \"Sinarcas\": (39.733258, -1.229035), \n",
    "    \"Sollana\": (39.278009, -0.381457), \n",
    "    \"Sot de Chera\": (39.620816, -0.909344), \n",
    "    \"Sueca\": (39.202640, -0.310637),\n",
    "    \"Tavernes de la Valldigna\": (39.071834, -0.267740), \n",
    "    \"Torrent\": (39.436931, -0.465889), \n",
    "    \"Tous\": (39.138561, -0.586636), \n",
    "    \"Turís\": (39.389834, -0.711107), \n",
    "    \"Utiel\": (39.566851, -1.206009), \n",
    "    \"València\": (39.469844, -0.376852), \n",
    "    \"Vilamarxant\": (39.567855, -0.622488), \n",
    "    \"Xirivella\": (39.463360, -0.428399),\n",
    "    \"Mira\": (39.720949, -1.439225)\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:44:21,868 [INFO] Loading saved study area polygon...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(polygon_path):\n",
    "    logging.info(\"Loading saved study area polygon...\")\n",
    "    study_area = gpd.read_file(polygon_path)\n",
    "else:\n",
    "    logging.info(\"Downloading polygons for municipalities...\")\n",
    "    polygons = []\n",
    "\n",
    "    for municipality in valencia_municipalities:\n",
    "        try:\n",
    "            place_name = f\"{municipality}, Valencia, Spain\"\n",
    "            gdf = ox.geocode_to_gdf(place_name)\n",
    "\n",
    "            if gdf.crs != \"EPSG:4326\":\n",
    "                gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "            polygons.append(gdf)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error retrieving {municipality}: {e}\", exc_info=True)\n",
    "\n",
    "    # Add Mira from Cuenca (just in case)\n",
    "    try:\n",
    "        gdf = ox.geocode_to_gdf(\"Mira, Cuenca, Spain\")\n",
    "        if gdf.crs != \"EPSG:4326\":\n",
    "            gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "        polygons.append(gdf)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error retrieving Mira: {e}\", exc_info=True)\n",
    "\n",
    "    study_area = gpd.GeoDataFrame(pd.concat(polygons, ignore_index=True), crs=\"EPSG:4326\")\n",
    "    study_area = study_area[study_area.geometry.notnull()]\n",
    "    study_area.to_file(polygon_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:44:22,165 [INFO] Polygon geometry union complete.\n"
     ]
    }
   ],
   "source": [
    "study_area = study_area[study_area.geometry.type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "polygon = unary_union(study_area.geometry)\n",
    "logging.info(\"Polygon geometry union complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:44:22,522 [INFO] Loading saved road network graph...\n",
      "2025-07-03 12:44:33,126 [INFO] Converted graph to GeoDataFrames.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(graph_path):\n",
    "    logging.info(\"Loading saved road network graph...\")\n",
    "    G = ox.load_graphml(graph_path)\n",
    "    \n",
    "else:\n",
    "    logging.info(\"Downloading road network...\")\n",
    "\n",
    "    G = ox.graph_from_polygon(\n",
    "        polygon,\n",
    "        network_type=\"drive\",\n",
    "        simplify=True,\n",
    "        retain_all=False,\n",
    "        truncate_by_edge=True\n",
    "    )\n",
    "\n",
    "    logging.info(\"Calculating travel times...\")\n",
    "    for u, v, k, data in G.edges(keys=True, data=True):\n",
    "        if \"length\" in data:\n",
    "            speed = None\n",
    "\n",
    "            if \"maxspeed\" in data:\n",
    "                maxspeed = data[\"maxspeed\"]\n",
    "                if isinstance(maxspeed, list):\n",
    "                    speed = maxspeed[0]\n",
    "                else:\n",
    "                    speed = maxspeed\n",
    "\n",
    "                try:\n",
    "                    speed = float(str(maxspeed).split()[0])  # Handle \"50 km/h\" etc.\n",
    "                except ValueError:\n",
    "                    speed = None  # Fallback to highway-based speed below\n",
    "\n",
    "            if speed is None:\n",
    "                highway = data.get(\"highway\", \"\")\n",
    "                if isinstance(highway, list):\n",
    "                    highway = highway[0]\n",
    "                speed = {\n",
    "                    \"motorway\": 120,\n",
    "                    \"motorway_link\": 60,\n",
    "                    \"trunk\": 100,\n",
    "                    \"primary\": 80,\n",
    "                    \"secondary\": 60,\n",
    "                    \"tertiary\": 50,\n",
    "                    \"residential\": 30,\n",
    "                    \"living_street\": 10,\n",
    "                    \"unclassified\": 40,\n",
    "                    \"service\": 20\n",
    "                }.get(highway, 50)  # fallback 50 km/h\n",
    "\n",
    "            surface = data.get(\"surface\", \"\").lower()\n",
    "            surface_speed_factor = {\n",
    "                \"paved\": 1.0,\n",
    "                \"asphalt\": 1.0,\n",
    "                \"concrete\": 1.0,\n",
    "                \"cobblestone\": 0.8,\n",
    "                \"gravel\": 0.7,\n",
    "                \"dirt\": 0.6,\n",
    "                \"ground\": 0.6,\n",
    "                \"sand\": 0.5,\n",
    "                \"unpaved\": 0.7,\n",
    "                \"compacted\": 0.85,\n",
    "                \"fine_gravel\": 0.9\n",
    "            }\n",
    "            for key, factor in surface_speed_factor.items():\n",
    "                if key in surface:\n",
    "                    speed *= factor\n",
    "                    break \n",
    "\n",
    "            speed_mps = speed * 1000 / 3600\n",
    "            data[\"travel_time\"] = data[\"length\"] / speed_mps\n",
    "            data[\"travel_time\"] += 5 # Turn penalty approximation\n",
    "\n",
    "    ox.save_graphml(G, filepath=graph_path)\n",
    "    logging.info(\"Graph saved.\")\n",
    "\n",
    "nodes, edges = ox.graph_to_gdfs(G)\n",
    "logging.info(\"Converted graph to GeoDataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:44:45,275 [INFO] Updated graph saved to: processed_files\\road_graph.graphml\n"
     ]
    }
   ],
   "source": [
    "node_coords = np.array([(geom.y, geom.x) for geom in nodes.geometry])\n",
    "kdtree = cKDTree(node_coords)\n",
    "\n",
    "if 'municipality' not in nodes.columns:\n",
    "    nodes['municipality'] = \"\"\n",
    "\n",
    "for name, (lat, lon) in urban_center.items():\n",
    "    try:\n",
    "        # Query closest node index\n",
    "        _, idx = kdtree.query([lat, lon], k=1)\n",
    "        nearest_node_idx = nodes.index[idx]\n",
    "        nodes.at[nearest_node_idx, 'municipality'] = name\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error assigning municipality {name}: {e}\")\n",
    "\n",
    "\n",
    "for node_id, row in nodes.iterrows():\n",
    "    G.nodes[node_id]['municipality'] = row['municipality']\n",
    "\n",
    "ox.save_graphml(G, graph_path)\n",
    "logging.info(f\"Updated graph saved to: {graph_path}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:44:45,295 [INFO] 'municipality' field already exists in nodes.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "if 'municipality' not in nodes.columns:\n",
    "    logging.info(\"Adding 'municipality' field to nodes...\")\n",
    "\n",
    "    nodes['municipality'] = \"\"\n",
    "\n",
    "    node_coords = np.array([(geom.y, geom.x) for geom in nodes.geometry])\n",
    "    kdtree = cKDTree(node_coords)\n",
    "\n",
    "    for _, row in study_area.iterrows():\n",
    "        name = row.get(\"name\") or row.get(\"display_name\") or \"unknown\"\n",
    "        geom = row.geometry\n",
    "\n",
    "        if not geom or not geom.is_valid or name == \"Favara\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            muni_graph = ox.graph_from_polygon(geom, network_type='drive', simplify=True)\n",
    "            center_node = ox.distance.nearest_nodes(muni_graph, X=geom.centroid.x, Y=geom.centroid.y)\n",
    "            center_point = Point((muni_graph.nodes[center_node]['x'], muni_graph.nodes[center_node]['y']))\n",
    "            _, idx = kdtree.query([center_point.y, center_point.x], k=1)\n",
    "            nearest_node_idx = nodes.index[idx]\n",
    "            nodes.at[nearest_node_idx, 'municipality'] = name\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not process {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    for node_id, row in nodes.iterrows():\n",
    "        G.nodes[node_id]['municipality'] = row['municipality']\n",
    "\n",
    "    graph_path = os.path.join(output_dir, \"road_graph.graphml\")\n",
    "    ox.save_graphml(G, graph_path)\n",
    "    logging.info(f\"Updated graph saved to: {graph_path}\")\n",
    "\n",
    "else:\n",
    "    logging.info(\"'municipality' field already exists in nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail=False\n",
    "if rail:\n",
    "    graph_path_rail = os.path.join(output_dir, \"study_area_rail.graphml\")\n",
    "    if os.path.exists(graph_path_rail):\n",
    "        print(\"Loading saved rail network graph...\")\n",
    "        G_rail = ox.load_graphml(graph_path_rail)\n",
    "    else:\n",
    "        print(\"Downloading rail network graph...\")\n",
    "        rail_filter = '[\"railway\"~\"rail|light_rail|subway|tram\"]'\n",
    "        G_rail = ox.graph_from_polygon(polygon, custom_filter=rail_filter, network_type=\"all\")\n",
    "        ox.save_graphml(G_rail, filepath=graph_path_rail)\n",
    "\n",
    "    nodes_rail, edges_rail = ox.graph_to_gdfs(G_rail)\n",
    "    del G_rail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"source_files\"\n",
    "output_dir = \"processed_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_input_files = {\n",
    "    \"10 yr\": f\"{input_dir}/laminaspb-q10/Q10_2Ciclo_PB_20241121.shp\",\n",
    "    \"100 yr\": f\"{input_dir}/laminaspb-q100/Q100_2Ciclo_PB_20241121_ETRS89.shp\",\n",
    "    \"500 yr\": f\"{input_dir}/laminaspb-q500/Q500_2Ciclo_PB_20241121_ETRS89.shp\",\n",
    "    \"DANA_31_10_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_PRODUCT_v1/EMSR773_AOI01_DEL_PRODUCT_observedEventA_v1.shp\",\n",
    "    \"DANA_03_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT01_v1/EMSR773_AOI01_DEL_MONIT01_observedEventA_v1.shp\",\n",
    "    \"DANA_05_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT02_v1/EMSR773_AOI01_DEL_MONIT02_observedEventA_v1.shp\",\n",
    "    \"DANA_06_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT03_v1/EMSR773_AOI01_DEL_MONIT03_observedEventA_v1.shp\",\n",
    "    \"DANA_08_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT04_v1/EMSR773_AOI01_DEL_MONIT04_observedEventA_v1.shp\"\n",
    "}\n",
    "\n",
    "depth_input_files = {\n",
    "    \"DANA_31_10_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_PRODUCT_v1/EMSR773_AOI01_DEL_PRODUCT_floodDepthA_v1.shp\",\n",
    "    \"DANA_03_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT01_v1/EMSR773_AOI01_DEL_MONIT01_floodDepthA_v1.shp\",\n",
    "    \"DANA_05_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT02_v1/EMSR773_AOI01_DEL_MONIT02_floodDepthA_v1.shp\",\n",
    "    \"DANA_06_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT03_v1/EMSR773_AOI01_DEL_MONIT03_floodDepthA_v1.shp\",\n",
    "    \"DANA_08_11_2024\": f\"{input_dir}/EMSR773_AOI01_DEL_MONIT04_v1/EMSR773_AOI01_DEL_MONIT04_floodDepthA_v1.shp\"\n",
    "}\n",
    "\n",
    "zone_output_files = {\n",
    "    \"10 yr\": f\"{output_dir}/zone_flood_risk_10.gpkg\",\n",
    "    \"100 yr\": f\"{output_dir}/zone_flood_risk_100.gpkg\",\n",
    "    \"500 yr\": f\"{output_dir}/zone_flood_risk_500.gpkg\",\n",
    "    \"DANA_31_10_2024\": f\"{output_dir}/zone_DANA_31_10_2024.gpkg\",\n",
    "    \"DANA_03_11_2024\": f\"{output_dir}/zone_DANA_03_11_2024.gpkg\",\n",
    "    \"DANA_05_11_2024\": f\"{output_dir}/zone_DANA_05_11_2024.gpkg\",\n",
    "    \"DANA_06_11_2024\": f\"{output_dir}/zone_DANA_06_11_2024.gpkg\",\n",
    "    \"DANA_08_11_2024\": f\"{output_dir}/zone_DANA_08_11_2024.gpkg\"\n",
    "}\n",
    "\n",
    "depth_output_files = {\n",
    "    \"DANA_31_10_2024\": f\"{output_dir}/depth_DANA_31_10_2024.gpkg\",\n",
    "    \"DANA_03_11_2024\": f\"{output_dir}/depth_DANA_03_11_2024.gpkg\",\n",
    "    \"DANA_05_11_2024\": f\"{output_dir}/depth_DANA_05_11_2024.gpkg\",\n",
    "    \"DANA_06_11_2024\": f\"{output_dir}/depth_DANA_06_11_2024.gpkg\",\n",
    "    \"DANA_08_11_2024\": f\"{output_dir}/depth_DANA_08_11_2024.gpkg\"\n",
    "}\n",
    "\n",
    "cut_roads_files = {\n",
    "    \"10 yr\": f\"{output_dir}/cut_roads_flood_risk_10.graphml\",\n",
    "    \"100 yr\": f\"{output_dir}/cut_roads_flood_risk_100.graphml\",\n",
    "    \"500 yr\": f\"{output_dir}/cut_roads_flood_risk_500.graphml\",\n",
    "    \"DANA_31_10_2024\": f\"{output_dir}/cut_roads_DANA_31_10_2024.graphml\",\n",
    "    \"DANA_03_11_2024\": f\"{output_dir}/cut_roads_DANA_03_11_2024.graphml\",\n",
    "    \"DANA_05_11_2024\": f\"{output_dir}/cut_roads_DANA_05_11_2024.graphml\",\n",
    "    \"DANA_06_11_2024\": f\"{output_dir}/cut_roads_DANA_06_11_2024.graphml\",\n",
    "    \"DANA_08_11_2024\": f\"{output_dir}/cut_roads_DANA_08_11_2024.graphml\"\n",
    "}\n",
    "\n",
    "safe_roads_files = {\n",
    "    \"10 yr\": f\"{output_dir}/safe_roads_flood_risk_10.graphml\",\n",
    "    \"100 yr\": f\"{output_dir}/safe_roads_flood_risk_100.graphml\",\n",
    "    \"500 yr\": f\"{output_dir}/safe_roads_flood_risk_500.graphml\",\n",
    "    \"DANA_31_10_2024\": f\"{output_dir}/safe_roads_DANA_31_10_2024.graphml\",\n",
    "    \"DANA_03_11_2024\": f\"{output_dir}/safe_roads_DANA_03_11_2024.graphml\",\n",
    "    \"DANA_05_11_2024\": f\"{output_dir}/safe_roads_DANA_05_11_2024.graphml\",\n",
    "    \"DANA_06_11_2024\": f\"{output_dir}/safe_roads_DANA_06_11_2024.graphml\",\n",
    "    \"DANA_08_11_2024\": f\"{output_dir}/safe_roads_DANA_08_11_2024.graphml\"\n",
    "}\n",
    "\n",
    "shortest_path_files = {\n",
    "    \"Normal Conditions\": f\"{output_dir}/shorthest_paths_NP.json\", \n",
    "    \"10 yr\": f\"{output_dir}/shorthest_paths_10.json\",\n",
    "    \"100 yr\": f\"{output_dir}/shorthest_paths_100.json\",\n",
    "    \"500 yr\": f\"{output_dir}/shorthest_paths_500.json\",\n",
    "    \"DANA_31_10_2024\": f\"{output_dir}/shorthest_paths_DANA_31_10_2024.json\",\n",
    "    \"DANA_03_11_2024\": f\"{output_dir}/shorthest_paths_DANA_03_11_2024.json\",\n",
    "    \"DANA_05_11_2024\": f\"{output_dir}/shorthest_paths_DANA_05_11_2024.json\",\n",
    "    \"DANA_06_11_2024\": f\"{output_dir}/shorthest_paths_DANA_06_11_2024.json\",\n",
    "    \"DANA_08_11_2024\": f\"{output_dir}/shorthest_paths_DANA_08_11_2024.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im671dntApI-"
   },
   "source": [
    "# Flood zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_names=[\"10 yr\",\"100 yr\",\"500 yr\",\"DANA_31_10_2024\",\"DANA_03_11_2024\",\"DANA_05_11_2024\",\"DANA_06_11_2024\",\"DANA_08_11_2024\"]\n",
    "layer_names=[\"DANA_31_10_2024\"]\n",
    "flood_zones_var = {}\n",
    "flood_edges_var = {}\n",
    "flood_graph_var = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DANA_31_10_2024 from processed_files/zone_DANA_31_10_2024.gpkg\n",
      "Loading DANA_31_10_2024 from processed_files/cut_roads_DANA_31_10_2024.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\anaconda3\\envs\\oxenv\\lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: File processed_files/cut_roads_DANA_31_10_2024.graphml has GPKG application_id, but non conformant file extension\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned graph already exists\n"
     ]
    }
   ],
   "source": [
    "for name in layer_names: \n",
    "    result = clip_flood_zone(edges.crs, name, polygon)\n",
    "    flood_zones_var[name] = result\n",
    "    result_1, result_2 = tag_flooded_roads(edges, nodes, result, name)\n",
    "    flood_edges_var[name] = result_1\n",
    "    flood_graph_var[name] = result_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [\"DANA_31_10_2024\",\"DANA_03_11_2024\",\"DANA_05_11_2024\",\"DANA_06_11_2024\",\"DANA_08_11_2024\"]\n",
    "depth_zones = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading depth_val from processed_files/depth_DANA_31_10_2024.gpkg\n",
      "Loading depth_val from processed_files/depth_DANA_03_11_2024.gpkg\n",
      "Loading depth_val from processed_files/depth_DANA_05_11_2024.gpkg\n",
      "Loading depth_val from processed_files/depth_DANA_06_11_2024.gpkg\n",
      "Loading depth_val from processed_files/depth_DANA_08_11_2024.gpkg\n"
     ]
    }
   ],
   "source": [
    "for name in layer_names: \n",
    "    depth = flood_depth_zones(name)\n",
    "    depth_zones[name]=depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navegability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = shortest_path_files[\"Normal Conditions\"]\n",
    "\n",
    "special_nodes = [n for n, attr in G.nodes(data=True) if attr.get('municipality')]\n",
    "node_to_muni = {n: attr.get('municipality', '') for n, attr in G.nodes(data=True)}\n",
    "\n",
    "T_NP_dictionary = load_or_compute_shortest_paths(filename, G, special_nodes, node_to_muni,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANA_31_10_2024 22 weakly connected components\n",
      "Component 1 municipalities: {'Albal', 'Montroi', 'Silla', 'Calles', 'Fuenterrobles', 'Utiel', 'Alcàsser', 'Pedralba', 'Chulilla', 'Benifaió', 'Llombai', 'Real', 'Massanassa', 'Alfarb', 'Benimodo', 'Torrent', 'Almussafes', 'Sollana', 'Picassent', 'Carlet', 'Alborache', 'Buñol', 'Llíria', 'Siete Aguas', 'Chiva', 'Millares', 'Benetússer', 'Riba-roja de Túria', 'Dos Aguas', 'Mislata', 'Gestalgar', 'Cheste', 'Sedaví', 'Catadau', 'Sot de Chera', 'Bétera', 'Alginet', 'Alfafar', 'Chera', 'Paterna', 'Bugarra', 'Requena', 'Aldaia', 'Godelleta', 'Vilamarxant', 'Tous', 'Macastre', 'Manises', 'Loriguilla', 'Montserrat', 'Turís', 'València', 'Alaquàs', 'Caudete de las Fuentes', 'Quart de Poblet', 'Catarroja'}\n",
      "Component 2 municipalities: {'Algemesí'}\n",
      "Component 3 municipalities: {'Carcaixent', \"l'Ènova\", 'Tavernes de la Valldigna', 'Alzira', \"l'Alcúdia\", 'Favara', 'Corbera', 'Cullera', 'Llaurí', 'Manuel'}\n",
      "Component 4 municipalities: {'Sueca'}\n",
      "Component 5 municipalities: {'Albalat de la Ribera'}\n",
      "Component 6 municipalities: {'Riola'}\n",
      "Component 7 municipalities: {'Polinyà de Xúquer'}\n",
      "Component 8 municipalities: {'Guadassuar'}\n",
      "Component 9 municipalities: {'Fortaleny'}\n",
      "Component 10 municipalities: {'Senyera'}\n",
      "Component 11 municipalities: {'Sinarcas'}\n",
      "Component 12 municipalities: {'Camporrobles'}\n",
      "Component 13 municipalities: {'Mira'}\n",
      "Component 14 municipalities: {'Xirivella'}\n",
      "Component 15 municipalities: {'Picanya'}\n",
      "Component 16 municipalities: {'Benicull de Xúquer'}\n",
      "Component 17 municipalities: {'Llocnou de la Corona'}\n",
      "Component 18 municipalities: {'Paiporta'}\n",
      "Component 19 municipalities: {'Rafelguaraf'}\n",
      "Component 20 municipalities: {'La Pobla Llarga'}\n",
      "Component 21 municipalities: {'Alberic'}\n",
      "Component 22 municipalities: {'Beniparrell'}\n"
     ]
    }
   ],
   "source": [
    "for name, graph in flood_graph_var.items():\n",
    "    components = list(nx.weakly_connected_components(graph))\n",
    "    \n",
    "    multi_muni_count = sum(\n",
    "        1 for component in components\n",
    "        if len({graph.nodes[node][\"municipality\"] for node in component if graph.nodes[node][\"municipality\"] != \"\"}) > 0\n",
    "    )\n",
    "\n",
    "    print(f\"{name} {multi_muni_count} weakly connected components\")\n",
    "\n",
    "    if multi_muni_count > 0:\n",
    "        j = 0\n",
    "        for i, component in enumerate(components):\n",
    "            municipalities = {\n",
    "                graph.nodes[node][\"municipality\"]\n",
    "                for node in component\n",
    "                if graph.nodes[node][\"municipality\"] != \"\"\n",
    "            }\n",
    "            if len(municipalities) > 0:\n",
    "                j += 1\n",
    "                print(f\"Component {j} municipalities: {municipalities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_names=[\"10 yr\",\"100 yr\",\"500 yr\",\"DANA_31_10_2024\",\"DANA_03_11_2024\",\"DANA_05_11_2024\",\"DANA_06_11_2024\",\"DANA_08_11_2024\"]\n",
    "layer_names=[\"DANA_31_10_2024\"]\n",
    "T_P_dictionaries = {}\n",
    "R={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00% (1/1)"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(layer_names):\n",
    "    T_P_dictionaries[name] = load_or_compute_shortest_paths(shortest_path_files[name], flood_graph_var[name], special_nodes, node_to_muni,True,True)\n",
    "    R[name] = compute_risk_factor(T_P_dictionaries[name], T_NP_dictionary)\n",
    "    \n",
    "    percent_complete = (i + 1) / len(layer_names) * 100\n",
    "    print(f\"\\rProgress: {percent_complete:.2f}% ({i + 1}/{len(layer_names)})\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_edges = set()\n",
    "\n",
    "for path, _ in T_NP_dictionary.values():\n",
    "    for u, v in zip(path, path[1:]):\n",
    "        used_edges.add((u, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (1/100)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_17580\\172459596.py:7: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "  dist_matrix = G_ig.shortest_paths_dijkstra(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00% (100/100) "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         2031257 function calls in 60.028 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "      100   57.525    0.575   57.525    0.575 {method 'distances' of 'igraph._igraph.GraphBase' objects}\n",
       "      100    0.718    0.007    0.718    0.007 {function _delete_edges at 0x00000222E99D1120}\n",
       "      101    0.521    0.005    0.521    0.005 {function _add_edges at 0x00000222E99D0F70}\n",
       "      100    0.463    0.005    0.547    0.005 426513350.py:1(compute_risk_factor_2)\n",
       "      100    0.302    0.003   57.896    0.579 172459596.py:1(batch_shortest_paths_no_path)\n",
       "        1    0.088    0.088    0.220    0.220 3831119956.py:1(convert_nx_to_igraph)\n",
       "   731000    0.083    0.000    0.083    0.000 4263211823.py:1(compute_individual_risk_factor)\n",
       "   124611    0.071    0.000    0.100    0.000 reportviews.py:958(<genexpr>)\n",
       "      804    0.057    0.000    0.057    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "        1    0.047    0.047   60.028   60.028 <string>:1(<module>)\n",
       "   731000    0.031    0.000    0.031    0.000 {built-in method math.isinf}\n",
       "      100    0.027    0.000   57.553    0.576 structural.py:86(_shortest_paths)\n",
       "   182252    0.018    0.000    0.020    0.000 {method 'items' of 'dict' objects}\n",
       "   124610    0.012    0.000    0.012    0.000 {method 'get' of 'dict' objects}\n",
       "      100    0.009    0.000    0.727    0.007 basic.py:130(_delete_edges)\n",
       "   124610    0.009    0.000    0.009    0.000 reportviews.py:934(<lambda>)\n",
       "      304    0.007    0.000    0.007    0.000 socket.py:623(send)\n",
       "      100    0.006    0.000    0.006    0.000 172459596.py:5(<dictcomp>)\n",
       "        1    0.005    0.005    0.005    0.005 3831119956.py:3(<dictcomp>)\n",
       "      101    0.004    0.000    0.526    0.005 basic.py:31(_add_edges)\n",
       "      100    0.004    0.000    0.004    0.000 172459596.py:4(<listcomp>)\n",
       "        1    0.004    0.004    0.004    0.004 3831119956.py:4(<dictcomp>)\n",
       "      201    0.002    0.000    0.011    0.000 iostream.py:655(write)\n",
       "      304    0.001    0.000    0.009    0.000 iostream.py:259(schedule)\n",
       "      100    0.001    0.000    0.001    0.000 {built-in method _warnings.warn}\n",
       "      100    0.001    0.000    0.063    0.001 iostream.py:592(flush)\n",
       "      348    0.001    0.000    0.002    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
       "      100    0.001    0.000    0.059    0.001 threading.py:589(wait)\n",
       "      100    0.001    0.000    0.001    0.000 threading.py:236(__init__)\n",
       "      100    0.001    0.000    0.058    0.001 threading.py:288(wait)\n",
       "      100    0.001    0.000    0.012    0.000 {built-in method builtins.print}\n",
       "      404    0.001    0.000    0.001    0.000 threading.py:1169(is_alive)\n",
       "      200    0.001    0.000    0.001    0.000 {built-in method _thread.allocate_lock}\n",
       "      174    0.000    0.000    0.001    0.000 threading.py:1478(enumerate)\n",
       "     1418    0.000    0.000    0.000    0.000 threading.py:1145(ident)\n",
       "      174    0.000    0.000    0.001    0.000 ipkernel.py:790(<setcomp>)\n",
       "      100    0.000    0.000    0.000    0.000 {method 'get_eid' of 'igraph._igraph.GraphBase' objects}\n",
       "      304    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
       "      201    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
       "      201    0.000    0.000    0.008    0.000 iostream.py:577(_schedule_flush)\n",
       "      100    0.000    0.000    0.001    0.000 utils.py:28(deprecated)\n",
       "        1    0.000    0.000   60.028   60.028 {built-in method builtins.exec}\n",
       "      101    0.000    0.000    0.000    0.000 __init__.py:590(es)\n",
       "      404    0.000    0.000    0.000    0.000 threading.py:1102(_wait_for_tstate_lock)\n",
       "      201    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
       "      100    0.000    0.000    0.001    0.000 threading.py:545(__init__)\n",
       "      100    0.000    0.000    0.000    0.000 threading.py:267(__exit__)\n",
       "      100    0.000    0.000    0.000    0.000 threading.py:276(_acquire_restore)\n",
       "        1    0.000    0.000    0.000    0.000 {function _add_vertices at 0x00000222E99D1090}\n",
       "      508    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "      898    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "      202    0.000    0.000    0.000    0.000 {method 'ecount' of 'igraph._igraph.GraphBase' objects}\n",
       "      100    0.000    0.000    0.000    0.000 threading.py:1430(current_thread)\n",
       "      201    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
       "      100    0.000    0.000    0.000    0.000 threading.py:264(__enter__)\n",
       "      375    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
       "      404    0.000    0.000    0.000    0.000 threading.py:553(is_set)\n",
       "      553    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "      201    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
       "      404    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "      100    0.000    0.000    0.000    0.000 threading.py:279(_is_owned)\n",
       "      100    0.000    0.000    0.000    0.000 threading.py:273(_release_save)\n",
       "      201    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
       "      349    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "      100    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <string>:2(<listcomp>)\n",
       "      100    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:349(__init__)\n",
       "      100    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
       "      100    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "      100    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
       "      100    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:35(_formatwarnmsg_impl)\n",
       "        1    0.000    0.000    0.000    0.000 reportviews.py:916(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 basic.py:84(_add_vertices)\n",
       "        1    0.000    0.000    0.000    0.000 reportviews.py:1371(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:403(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1053(_handle_fromlist)\n",
       "        1    0.000    0.000    0.000    0.000 reportviews.py:187(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 reportviews.py:957(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:36(getlines)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:26(getline)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'difference_update' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 reportviews.py:184(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 reportviews.py:207(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 multidigraph.py:865(is_directed)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%prun\n",
    "G_ig, node_to_index, index_to_node, edge_weights = convert_nx_to_igraph(G, weight_attr='travel_time')  # <<< CHANGED (save edge_weights)\n",
    "special_indices = [node_to_index[n] for n in special_nodes]\n",
    "\n",
    "base_risk = 0\n",
    "edge_risks = []\n",
    "i=0\n",
    "total = len(used_edges)\n",
    "#total = 100\n",
    "\n",
    "for u, v in list(used_edges)[:total]:\n",
    "    i+=1\n",
    "    u_idx, v_idx = node_to_index[u], node_to_index[v]\n",
    "    eid = G_ig.get_eid(u_idx, v_idx, directed=True, error=False)  # <<< NEW (get edge id in igraph)\n",
    "    if eid == -1:\n",
    "        continue\n",
    "\n",
    "    # Remove edge in-place\n",
    "    G_ig.delete_edges(eid)  # <<< NEW (remove edge without copying)\n",
    "\n",
    "    # Compute shortest paths after removal (without full paths)\n",
    "    T_P_dictionary = batch_shortest_paths_no_path(G_ig, special_nodes, index_to_node, node_to_index, node_to_muni)  # <<< CHANGED (use igraph)\n",
    "\n",
    "    new_risk = compute_risk_factor_2(T_P_dictionary, T_NP_dictionary)\n",
    "    delta_risk = new_risk - base_risk\n",
    "\n",
    "    edge_risks.append(((u, v), delta_risk))\n",
    "\n",
    "    # Re-add the edge with original weight\n",
    "    G_ig.add_edges([(u_idx, v_idx)])  # <<< NEW (re-add edge)\n",
    "    G_ig.es[-1]['weight'] = edge_weights[(u_idx, v_idx)]  # <<< NEW (set original weight)\n",
    "\n",
    "    # Print progress (overwrite line)\n",
    "    percent_complete = (i) / total * 100\n",
    "    print(f\"\\rProgress: {percent_complete:.2f}% ({i}/{total})\", end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "edge_risks.sort(key=lambda x: x[1], reverse=True)\n",
    "edge_risks_json = [ {\"edge\": [u, v], \"delta_risk\": delta_risk} for (u, v), delta_risk in edge_risks ]\n",
    "with open(\"processed_files/edge_risks_NP.json\", \"w\") as f:\n",
    "    json.dump(edge_risks_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00% (17464/17464)CPU times: total: 4h 18min 3s\n",
      "Wall time: 4h 21min 34s\n"
     ]
    }
   ],
   "source": [
    "%%prun\n",
    "\n",
    "# Step 1: Convert reduced graph to igraph\n",
    "G_ig_dana, node_to_index, index_to_node, _ = convert_nx_to_igraph(\n",
    "    flood_graph_var[\"DANA_31_10_2024\"], weight_attr='travel_time')\n",
    "\n",
    "# Step 2: Prepare special node indices\n",
    "special_indices = [node_to_index[n] for n in special_nodes]\n",
    "\n",
    "# Step 3: Compute baseline risk for reduced graph\n",
    "T_P_dictionary = batch_shortest_paths(\n",
    "    G_ig_dana, special_nodes, index_to_node, node_to_index, node_to_muni, compute_path=False)\n",
    "\n",
    "base_risk = compute_risk_factor(\n",
    "    T_P_dictionaries[\"DANA_31_10_2024\"], T_NP_dictionary)\n",
    "\n",
    "# Step 4: Prepare candidate edges (only in flood zone)\n",
    "candidate_edges = flood_edges_var[\"DANA_31_10_2024\"]\n",
    "candidate_edges = candidate_edges[candidate_edges['in_flood_zone'].astype(bool)].copy()\n",
    "\n",
    "edge_risks = []\n",
    "total = len(candidate_edges)\n",
    "\n",
    "# Step 5: Loop through candidate edges and evaluate impact\n",
    "for i, (_, row) in enumerate(candidate_edges.iterrows()):\n",
    "    u, v = row['u'], row['v']\n",
    "    weight = row['travel_time']\n",
    "\n",
    "    if u not in node_to_index or v not in node_to_index:\n",
    "        continue\n",
    "\n",
    "    u_idx, v_idx = node_to_index[u], node_to_index[v]\n",
    "\n",
    "    # Add edge\n",
    "    G_ig_dana.add_edges([(u_idx, v_idx)])\n",
    "    eid = G_ig_dana.get_eid(u_idx, v_idx, directed=True, error=False)\n",
    "    if eid == -1:\n",
    "        continue\n",
    "\n",
    "    G_ig_dana.es[eid]['weight'] = weight\n",
    "\n",
    "    # Recompute risk\n",
    "    T_P_dictionary = batch_shortest_paths(\n",
    "        G_ig_dana, special_nodes, index_to_node, node_to_index, node_to_muni, compute_path=False)\n",
    "    new_risk = compute_risk_factor_2(T_P_dictionary, T_NP_dictionary)\n",
    "    delta_risk = new_risk-base_risk\n",
    "\n",
    "    edge_risks.append(((u, v), delta_risk))\n",
    "\n",
    "    # Remove the edge\n",
    "    G_ig_dana.delete_edges(eid)\n",
    "\n",
    "    # Print progress (overwrite line)\n",
    "    percent_complete = (i + 1) / total * 100\n",
    "    print(f\"\\rProgress: {percent_complete:.2f}% ({i + 1}/{total})\", end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "edge_risks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "edge_risks_json = [ {\"edge\": [u, v], \"delta_risk\": delta_risk} for (u, v), delta_risk in edge_risks ]\n",
    "with open(\"processed_files/edge_risks_DANA.json\", \"w\") as f:\n",
    "    json.dump(edge_risks_json, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = dict(sorted(R.items(), key=lambda item: item[1]))\n",
    "keys = list(R.keys())\n",
    "values = list(R.values())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(keys, values, color=\"Blue\", alpha=0.5)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('R')\n",
    "plt.title('Risk Factor')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('processed_files/Risk_Factor.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = {\n",
    "    # yr values — warm and spaced\n",
    "    \"10 yr\": \"#FFD700\",    # Gold\n",
    "    \"100 yr\": \"#FF7F00\",   # Dark Orange\n",
    "    \"500 yr\": \"#B22222\",   # Firebrick \n",
    "\n",
    "    # DANA values — distinct, avoiding orange/red hues\n",
    "    \"DANA_31_10_2024\": \"#8A2BE2\",  # Blue-Violet\n",
    "    \"DANA_03_11_2024\": \"#FF1493\",  # Deep Pink\n",
    "    \"DANA_05_11_2024\": \"#00CED1\",  # Dark Turquoise\n",
    "    \"DANA_06_11_2024\": \"#32CD32\",  # Lime Green\n",
    "    \"DANA_08_11_2024\": \"#1E90FF\",  # Dodger Blue\n",
    "\n",
    "    # Normal condition\n",
    "    \"Normal Conditions\": \"#808080\"  # Grey\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# --- Prepare data ---\n",
    "plot_data = []\n",
    "zero_counts = {}\n",
    "all_counts = {}\n",
    "layer_names = [\"10 yr\", \"100 yr\", \"500 yr\"]\n",
    "\n",
    "# T_P_lists scenarios\n",
    "for name, results_dict in T_P_dictionaries.items():\n",
    "    if name in layer_names:\n",
    "        travel_times = [v[1] for v in results_dict.values()]\n",
    "        non_zero_times = [t / 60 for t in travel_times if t > 0]\n",
    "        all_times = [t / 60 for t in travel_times]\n",
    "        zero_counts[name] = travel_times.count(0)\n",
    "        all_counts[name] = len(travel_times)\n",
    "        plot_data.append((name, non_zero_times))\n",
    "\n",
    "# Add normal scenario\n",
    "normal_travel_times = [v[1] for v in T_NP_dictionary.values()]\n",
    "normal_non_zero = [t / 60 for t in normal_travel_times if t > 0]\n",
    "normal_total = len(normal_travel_times)\n",
    "plot_data.append((\"Normal Conditions\", normal_non_zero))\n",
    "all_counts[\"Normal Conditions\"] = normal_total\n",
    "zero_counts[\"Normal Conditions\"] = normal_travel_times.count(0)\n",
    "\n",
    "# --- KDE Plot ---\n",
    "fig, (ax_kde, ax_bar) = plt.subplots(1, 2, figsize=(14, 7), gridspec_kw={'width_ratios': [2.5, 1]})\n",
    "\n",
    "x_vals = np.linspace(0, max([max(times) if times else 0 for _, times in plot_data]), 500)\n",
    "\n",
    "for name, times in plot_data:\n",
    "    if len(times) > 1:\n",
    "        kde = gaussian_kde(times)\n",
    "        y = kde(x_vals)\n",
    "        ratio = len(times) / all_counts[name] if all_counts[name] > 0 else 0\n",
    "        y_rescaled = y * ratio\n",
    "        color = color_palette.get(name, 'gray')  # fallback to gray if not defined\n",
    "        ax_kde.plot(x_vals, y_rescaled, label=name, color=color)\n",
    "\n",
    "ax_kde.set_title(\"PDF of Travel Times\")\n",
    "ax_kde.set_xlabel(\"Travel Time (minutes)\")\n",
    "ax_kde.grid(True)\n",
    "ax_kde.legend(loc='upper right')\n",
    "\n",
    "# --- Reachability Bar Chart ---\n",
    "scenarios = layer_names + [\"Normal Conditions\"]\n",
    "reachable = [(all_counts[n] - zero_counts[n]) / all_counts[n] if all_counts[n] > 0 else 0 for n in scenarios]\n",
    "unreachable = [1 - r for r in reachable]\n",
    "\n",
    "bar_positions = np.arange(len(scenarios))\n",
    "ax_bar.barh(bar_positions, unreachable, color='salmon', label='Unreachable')\n",
    "ax_bar.barh(bar_positions, reachable, left=unreachable, color='mediumseagreen', label='Reachable')\n",
    "\n",
    "ax_bar.set_yticks(bar_positions)\n",
    "ax_bar.set_yticklabels(scenarios)\n",
    "ax_bar.set_xlim(0, 1)\n",
    "ax_bar.set_title(\"Fraction of cut routes\")\n",
    "ax_bar.set_xlabel(\"Proportion\")\n",
    "#ax_bar.legend(loc='lower right')\n",
    "ax_bar.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax_bar.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('processed_files/Travel_times_yr.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Prepare data ---\n",
    "plot_data = []\n",
    "zero_counts = {}\n",
    "all_counts = {}\n",
    "layer_names = [\"DANA_31_10_2024\",\"DANA_03_11_2024\",\"DANA_05_11_2024\",\"DANA_06_11_2024\",\"DANA_08_11_2024\"]\n",
    "\n",
    "# T_P_lists scenarios\n",
    "for name, results_dict in T_P_dictionaries.items():\n",
    "    if name in layer_names:\n",
    "        travel_times = [v[1] for v in results_dict.values()]\n",
    "        non_zero_times = [t / 60 for t in travel_times if t > 0]\n",
    "        all_times = [t / 60 for t in travel_times]\n",
    "        zero_counts[name] = travel_times.count(0)\n",
    "        all_counts[name] = len(travel_times)\n",
    "        plot_data.append((name, non_zero_times))\n",
    "\n",
    "# Add normal scenario\n",
    "normal_travel_times = [v[1] for v in T_NP_dictionary.values()]\n",
    "normal_non_zero = [t / 60 for t in normal_travel_times if t > 0]\n",
    "normal_total = len(normal_travel_times)\n",
    "plot_data.append((\"Normal Conditions\", normal_non_zero))\n",
    "all_counts[\"Normal Conditions\"] = normal_total\n",
    "zero_counts[\"Normal Conditions\"] = normal_travel_times.count(0)\n",
    "\n",
    "# --- KDE Plot ---\n",
    "fig, (ax_kde, ax_bar) = plt.subplots(1, 2, figsize=(14, 7), gridspec_kw={'width_ratios': [2.5, 1]})\n",
    "\n",
    "x_vals = np.linspace(0, max([max(times) if times else 0 for _, times in plot_data]), 500)\n",
    "\n",
    "for name, times in plot_data:\n",
    "    if len(times) > 1:\n",
    "        kde = gaussian_kde(times)\n",
    "        y = kde(x_vals)\n",
    "        ratio = len(times) / all_counts[name] if all_counts[name] > 0 else 0\n",
    "        y_rescaled = y * ratio\n",
    "        color = color_palette.get(name, 'gray')  # fallback to gray if not defined\n",
    "        ax_kde.plot(x_vals, y_rescaled, label=name, color=color)\n",
    "\n",
    "ax_kde.set_title(\"PDF of Travel Times\")\n",
    "ax_kde.set_xlabel(\"Travel Time (minutes)\")\n",
    "ax_kde.grid(True)\n",
    "ax_kde.legend(loc='upper right')\n",
    "\n",
    "# --- Reachability Bar Chart ---\n",
    "scenarios = layer_names + [\"Normal Conditions\"]\n",
    "reachable = [(all_counts[n] - zero_counts[n]) / all_counts[n] if all_counts[n] > 0 else 0 for n in scenarios]\n",
    "unreachable = [1 - r for r in reachable]\n",
    "\n",
    "bar_positions = np.arange(len(scenarios))\n",
    "ax_bar.barh(bar_positions, unreachable, color='salmon', label='Unreachable')\n",
    "ax_bar.barh(bar_positions, reachable, left=unreachable, color='mediumseagreen', label='Reachable')\n",
    "\n",
    "ax_bar.set_yticks(bar_positions)\n",
    "ax_bar.set_yticklabels(scenarios)\n",
    "ax_bar.set_xlim(0, 1)\n",
    "ax_bar.set_title(\"Fraction of cut routes\")\n",
    "ax_bar.set_xlabel(\"Proportion\")\n",
    "#ax_bar.legend(loc='lower right')\n",
    "ax_bar.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax_bar.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('processed_files/Travel_times_DANA.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59NZ-jYDAeVF"
   },
   "source": [
    "# Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial position\n",
    "projected = study_area.to_crs(epsg=25830)\n",
    "centroid_projected = projected.geometry.centroid.iloc[0]\n",
    "centroid_latlon = gpd.GeoSeries([centroid_projected], crs=25830).to_crs(epsg=4326).geometry.iloc[0]\n",
    "map_center = [centroid_latlon.y, centroid_latlon.x]\n",
    "bounds_wgs84 = study_area.to_crs(epsg=4326).total_bounds\n",
    "map_bounds = [[bounds_wgs84[1], bounds_wgs84[0]], [bounds_wgs84[3], bounds_wgs84[2]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areas at Risk and DANA Area (with roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1 = folium.Map(location=map_center, zoom_start=10, tiles=\"CartoDB positron\", max_bounds=True)\n",
    "m_1.fit_bounds(map_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(graph_path):\n",
    "    logging.info(\"Loading saved road network graph...\")\n",
    "    G = ox.load_graphml(graph_path)\n",
    "else:\n",
    "    logging.info(\"Downloading road network...\")\n",
    "    G = ox.graph_from_polygon(polygon, network_type=\"drive\", simplify=True)\n",
    "    ox.save_graphml(G, filepath=graph_path)\n",
    "    logging.info(\"Graph saved.\")\n",
    "\n",
    "nodes, edges = ox.graph_to_gdfs(G)\n",
    "logging.info(\"Converted graph to GeoDataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flood zones\n",
    "add_flood_zone_layer(\"10 yr\", m_1)\n",
    "add_flood_zone_layer(\"100 yr\", m_1)\n",
    "add_flood_zone_layer(\"500 yr\", m_1)\n",
    "add_flood_zone_layer(\"DANA_31_10_2024\", m_1)\n",
    "    \n",
    "# Add flooded roads (optional)\n",
    "add_roads_layer(\"10 yr\", m_1, True)\n",
    "add_roads_layer(\"100 yr\", m_1, True)\n",
    "add_roads_layer(\"500 yr\", m_1, True)\n",
    "add_roads_layer(\"DANA_31_10_2024\", m_1, True)\n",
    "add_roads_layer(\"Normal Conditions\", m_1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.LayerControl(collapsed=False).add_to(m_1)\n",
    "m_1.save(\"processed_files/Risk_max_DANA.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del m_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANA flood depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2 = folium.Map(location=map_center, zoom_start=10, tiles=\"CartoDB positron\", max_bounds=True)\n",
    "m_2.fit_bounds(map_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth=depth_zones[\"DANA_31_10_2024\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_depth = depth[\"depth_val\"].min()\n",
    "max_depth = depth[\"depth_val\"].max()\n",
    "depth_colormap = linear.YlGnBu_09.scale(min_depth, max_depth)\n",
    "depth_colormap.caption = 'Flood Depth (m)'\n",
    "\n",
    "folium.GeoJson(\n",
    "    depth,\n",
    "    name=\"DANA flood depth\",\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': depth_colormap(feature['properties']['depth_val']),\n",
    "        'color': 'black',\n",
    "        'weight': 0.5,\n",
    "        'fillOpacity': 0.7\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(fields=[\"depth_val\"], aliases=[\"Depth (m):\"])\n",
    ").add_to(m_2)\n",
    "\n",
    "depth_colormap.add_to(m_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.LayerControl(collapsed=False).add_to(m_2)\n",
    "m_2.save(\"processed_files/Max_flood_depth.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANA Flooded Area Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_3 = folium.Map(location=map_center, zoom_start=10, tiles=\"CartoDB positron\", max_bounds=True)\n",
    "m_3.fit_bounds(map_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flood zones\n",
    "add_flood_zone_layer(\"DANA_31_10_2024\", m_3)\n",
    "add_flood_zone_layer(\"DANA_03_11_2024\", m_3)\n",
    "add_flood_zone_layer(\"DANA_05_11_2024\", m_3)\n",
    "add_flood_zone_layer(\"DANA_06_11_2024\", m_3)\n",
    "add_flood_zone_layer(\"DANA_08_11_2024\", m_3)\n",
    "    \n",
    "# Add flooded roads (optional)\n",
    "add_roads_layer(\"DANA_31_10_2024\", m_3, True)\n",
    "add_roads_layer(\"DANA_03_11_2024\", m_3, True)\n",
    "add_roads_layer(\"DANA_05_11_2024\", m_3, True)\n",
    "add_roads_layer(\"DANA_06_11_2024\", m_3, True)\n",
    "add_roads_layer(\"DANA_08_11_2024\", m_3, True)\n",
    "add_roads_layer(\"Normal Conditions\", m_3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.LayerControl(collapsed=False).add_to(m_3)\n",
    "m_3.save(\"processed_files/DANA_evolution.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Ris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"processed_files/edge_risks_NP.json\", \"r\") as f:\n",
    "    delta_risks1 = json.load(f)\n",
    "with open(\"processed_files/edge_risks_DANA.json\", \"r\") as f:\n",
    "    delta_risks2 = json.load(f)\n",
    "\n",
    "# === Build delta dicts ===\n",
    "def build_delta_dict(delta_list):\n",
    "    return {\n",
    "        tuple(item['edge']): item['delta_risk']\n",
    "        for item in delta_list\n",
    "    }\n",
    "\n",
    "delta_dict1 = build_delta_dict(delta_risks1)\n",
    "delta_dict2 = build_delta_dict(delta_risks2)\n",
    "\n",
    "# === Signed log transform ===\n",
    "def signed_log_transform(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return 1/(np.sign(x) * np.log10(abs(x)))\n",
    "\n",
    "# === Get max absolute log ===\n",
    "all_values = list(delta_dict1.values()) + list(delta_dict2.values())\n",
    "transformed_values = [signed_log_transform(v) for v in all_values if v is not None]\n",
    "max_abs_log = max(abs(val) for val in transformed_values)\n",
    "\n",
    "# === Create custom diverging colormap ===\n",
    "def inverted_colormap(val):\n",
    "    \"\"\"\n",
    "    Maps log-transformed delta to color:\n",
    "    - stronger values (larger abs(val)) → stronger color (closer to red/blue)\n",
    "    - near-zero values → white\n",
    "    \"\"\"\n",
    "    if val == 0:\n",
    "        return \"#ffffff\"\n",
    "    \n",
    "    norm_val = signed_log_transform(val) / max_abs_log\n",
    "    abs_norm = abs(norm_val)\n",
    "\n",
    "    # invert strength: low abs → white, high abs → strong\n",
    "    strength = abs_norm\n",
    "    if norm_val > 0:\n",
    "        # red side\n",
    "        return cm.linear.Reds_09.scale(0, 1)(strength)\n",
    "    else:\n",
    "        # blue side\n",
    "        return cm.linear.Blues_09.scale(0, 1)(strength)\n",
    "\n",
    "# === Colorbar (caption only; not linked directly) ===\n",
    "legend_colormap = cm.LinearColormap(\n",
    "    colors=[\"blue\", \"white\", \"red\"],\n",
    "    vmin=-max_abs_log,\n",
    "    vmax=+max_abs_log\n",
    ")\n",
    "legend_colormap.caption = \"Δ Risk (Signed Log Scale, Stronger = Darker)\"\n",
    "\n",
    "# === Add edges to map ===\n",
    "def add_edges_to_map(fmap, edges, delta_dict, label, filter_flood=False):\n",
    "    fg = folium.FeatureGroup(name=label)\n",
    "\n",
    "    for idx, row in edges.iterrows():\n",
    "        edge_key = (row['u'], row['v']) if 'u' in row and 'v' in row else (idx[0], idx[1])\n",
    "\n",
    "        if filter_flood and not row.get(\"in_flood_zone\", False):\n",
    "            continue\n",
    "\n",
    "        value = delta_dict.get(edge_key)\n",
    "        if value is None:\n",
    "            continue\n",
    "\n",
    "        # Determine color\n",
    "        if value == 0:\n",
    "            color = \"#bbbbbb\"\n",
    "            opacity = 0.4\n",
    "            weight = 1\n",
    "        else:\n",
    "            color = inverted_colormap(value)\n",
    "            opacity = 0.8\n",
    "            weight = 2\n",
    "\n",
    "        coords = [(lat, lon) for lon, lat in row.geometry.coords]\n",
    "\n",
    "        folium.PolyLine(\n",
    "            locations=coords,\n",
    "            color=color,\n",
    "            weight=weight,\n",
    "            opacity=opacity,\n",
    "            tooltip=f\"{edge_key} | Δ Risk: {value:.1e}\"\n",
    "        ).add_to(fg)\n",
    "\n",
    "    fg.add_to(fmap)\n",
    "\n",
    "# === Build map ===\n",
    "m = folium.Map(location=map_center, zoom_start=10, tiles=\"CartoDB positron\", max_bounds=True)\n",
    "m.fit_bounds(map_bounds)\n",
    "\n",
    "add_edges_to_map(m, edges, delta_dict1, label=\"All Roads\")\n",
    "add_edges_to_map(m, flood_edges_var[\"DANA_31_10_2024\"], delta_dict2, label=\"DANA_31_10_2024\", filter_flood=True)\n",
    "\n",
    "m.add_child(legend_colormap)\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m.save(\"delta_risk_map.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (oxenv)",
   "language": "python",
   "name": "oxenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
